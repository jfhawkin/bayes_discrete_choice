library(rstan) # observe startup messages
library(dplyr)
rstan_options(auto_write = TRUE)

dat = read.csv('https://raw.githubusercontent.com/jfhawkin/bayes_discrete_choice/master/xmatswave2019.csv',header = FALSE)
# dat = cbind(dat, dummy(dat$ALT, sep = "_"))

# We have 8 tasks per individual for 184 individuals comparing 4 alternatives
I = 184  # Number of individuals
C = 424  # Number of data columns
K = 4    # Number of alternatives in each choice task
T = 8    # Number of alternatives by number of choice tasks
PN = 51   # One each for ASC, automation level
ASC = 4 # Number of alternatives/ASC 
P = ASC + PN # Total number of covariates

X = data.matrix(dat)

# Scale one time costs and monthly mileage to $1000/1000 mi
X[,71] = X[,71] / 1000
X[,73] = X[,73] / 1000
X[,102] = X[,102] / 1000
X[,104] = X[,104] / 1000
X[,116] = X[,116] / 1000
X[,118] = X[,118] / 1000
X[,147] = X[,147] / 1000
X[,149] = X[,149] / 1000
X[,160] = X[,160] / 1000
X[,161] = X[,161] / 1000
X[,163] = X[,163] / 1000
X[,192] = X[,194] / 1000
X[,205] = X[,205] / 1000
X[,206] = X[,206] / 1000
X[,208] = X[,208] / 1000
X[,237] = X[,237] / 1000
X[,251] = X[,251] / 1000
X[,253] = X[,253] / 1000
X[,284] = X[,284] / 1000
X[,295] = X[,295] / 1000
X[,296] = X[,296] / 1000
X[,298] = X[,298] / 1000
X[,312] = X[,312] / 1000
X[,327] = X[,327] / 1000
X[,340] = X[,340] / 1000
X[,341] = X[,341] / 1000
X[,343] = X[,343] / 1000
X[,372] = X[,371] / 1000
X[,374] = X[,374] / 1000
X[,385] = X[,385] / 1000
X[,386] = X[,386] / 1000
X[,388] = X[,388] / 1000
X[,417] = X[,417] / 1000
X[,419] = X[,419] / 1000
X[,87] =  X[,87] / 1000
X[,134] = X[,134] / 1000
X[,160] = X[,160] / 1000
X[,179] = X[,179] / 1000
X[,222] = X[,222] / 1000
X[,267] = X[,267] / 1000
X[,357] = X[,357] / 1000
X[,402] = X[,402] / 1000
# Scale monthly costs to $100
X[,90] = X[,90] / 100
X[,91] = X[,91] / 100
X[,93] = X[,93] / 100
X[,97] = X[,97] / 100
X[,135] = X[,135] / 100
X[,136] = X[,136] / 100
X[,138] = X[,138] / 100
X[,142] = X[,142] / 100
X[,180] = X[,180] / 100
X[,181] = X[,181] / 100
X[,183] = X[,183] / 100
X[,187] = X[,187] / 100
X[,224] = X[,224] / 100
X[,225] = X[,225] / 100
X[,226] = X[,226] / 100
X[,228] = X[,228] / 100
X[,232] = X[,232] / 100
X[,269] = X[,269] / 100
X[,270] = X[,270] / 100
X[,271] = X[,271] / 100
X[,273] = X[,273] / 100
X[,277] = X[,277] / 100
X[,315] = X[,315] / 100
X[,316] = X[,316] / 100
X[,318] = X[,318] / 100
X[,322] = X[,322] / 100
X[,359] = X[,359] / 100
X[,360] = X[,360] / 100
X[,361] = X[,361] / 100
X[,363] = X[,363] / 100
X[,367] = X[,367] / 100
X[,405] = X[,405] / 100
X[,406] = X[,406] / 100
X[,408] = X[,408] / 100
X[,412] = X[,412] / 100

choice = matrix(0L, nrow = I, ncol = T*K)
for(i in 1:T){
  for (j in 1:K){
    choice[,j+(i-1)*K] = as.integer(X[,49+(i-1)]==j)
  }
}

data_list = list(I = I,
                 C = C,
                 K = K,
                 T = T,
                 PN = PN,
                 ASC = ASC,
                 P = P,
                 X = X,
                 choice = choice
)

# Compile the model
compiled_model = stan_model("kaili_model_simp.stan")

# Fit the model
model_fit = sampling(compiled_model, data = data_list, iter = 2000, chains=4)

df = as.data.frame(model_fit)
write.csv(df,'kaili_model_simp.csv')

# sampler_params = get_sampler_params(model_fit, inc_warmup = TRUE)
# summary(do.call(rbind, sampler_params), digits = 2)
# print(model_fit)
# 
# accept_stat__    stepsize__       treedepth__    n_leapfrog__   divergent__       energy__    
# Min.   :0.00   Min.   :3.7e-04   Min.   : 0.0   Min.   :   1   Min.   :0.000   Min.   : 2496  
# 1st Qu.:0.81   1st Qu.:1.7e-02   1st Qu.: 8.0   1st Qu.: 255   1st Qu.:0.000   1st Qu.: 2589  
# Median :0.93   Median :1.8e-02   Median : 8.0   Median : 255   Median :0.000   Median : 2609  
# Mean   :0.85   Mean   :2.4e-02   Mean   : 7.8   Mean   : 280   Mean   :0.033   Mean   : 2671  
# 3rd Qu.:0.98   3rd Qu.:2.1e-02   3rd Qu.: 8.0   3rd Qu.: 255   3rd Qu.:0.000   3rd Qu.: 2629  
# Max.   :1.00   Max.   :1.4e+01   Max.   :10.0   Max.   :1023   Max.   :1.000   Max.   :50785  
# > print(model_fit)
# Inference for Stan model: kaili_model_simp.
# 4 chains, each with iter=2000; warmup=1000; thin=1; 
# post-warmup draws per chain=1000, total post-warmup draws=4000.
# 
# mean se_mean    sd     2.5%      25%      50%      75%    97.5% n_eff Rhat
# betan[1]           -0.07    0.00  0.01    -0.09    -0.08    -0.07    -0.07    -0.05  2859    1
# betan[2]            0.02    0.00  0.01     0.00     0.01     0.02     0.02     0.04  2404    1
# betan[3]           -0.02    0.00  0.11    -0.23    -0.09    -0.02     0.06     0.20  2380    1
# betan[4]           -0.07    0.01  0.37    -0.77    -0.32    -0.06     0.18     0.64  1428    1
# betan[5]           -0.10    0.00  0.11    -0.31    -0.17    -0.10    -0.03     0.11   995    1
# betan[6]           -0.02    0.00  0.07    -0.16    -0.07    -0.02     0.03     0.11  1032    1
# betan[7]            0.00    0.00  0.08    -0.16    -0.05     0.01     0.06     0.16  3126    1
# betan[8]            0.00    0.00  0.08    -0.16    -0.05     0.00     0.06     0.16  3170    1
# betan[9]            0.01    0.00  0.08    -0.16    -0.05     0.01     0.06     0.17  3256    1
# betan[10]           0.05    0.00  0.24    -0.42    -0.11     0.05     0.21     0.50  9924    1
# betan[11]          -0.03    0.06  3.54    -6.89    -2.50    -0.04     2.37     6.85  4021    1
# betan[12]          -0.01    0.06  3.52    -6.69    -2.40    -0.09     2.41     6.81  3999    1
# betan[13]           0.09    0.00  0.21    -0.33    -0.06     0.08     0.23     0.51  8019    1
# betan[14]           0.00    0.06  3.54    -6.86    -2.43     0.00     2.45     6.95  4014    1
# betan[15]           0.02    0.06  3.51    -6.91    -2.40     0.16     2.40     6.73  4040    1
# betan[16]           0.01    0.00  0.18    -0.34    -0.11     0.00     0.13     0.36  4299    1
# betan[17]          -0.03    0.00  0.18    -0.38    -0.15    -0.03     0.09     0.33  4501    1
# betan[18]          -0.09    0.00  0.18    -0.44    -0.20    -0.09     0.03     0.27  5176    1
# betan[19]          -0.02    0.00  0.17    -0.35    -0.14    -0.02     0.10     0.31  4319    1
# betan[20]          -0.02    0.00  0.18    -0.36    -0.14    -0.02     0.10     0.33  4116    1
# betan[21]           0.01    0.00  0.18    -0.33    -0.11     0.01     0.13     0.36  4571    1
# betan[22]          -0.04    0.00  0.18    -0.38    -0.15    -0.04     0.08     0.31  4221    1
# betan[23]           0.00    0.00  0.18    -0.36    -0.12     0.00     0.12     0.35  4379    1
# betan[24]           0.02    0.00  0.18    -0.32    -0.10     0.03     0.14     0.38  4017    1
# betan[25]           0.00    0.00  0.15    -0.29    -0.10     0.01     0.11     0.30  3189    1
# betan[26]           0.01    0.00  0.15    -0.28    -0.09     0.01     0.12     0.31  3229    1
# betan[27]           0.02    0.00  0.15    -0.28    -0.08     0.01     0.12     0.31  3487    1
# betan[28]           0.13    0.01  0.41    -0.69    -0.14     0.13     0.41     0.95  4681    1
# betan[29]          -0.07    0.01  0.41    -0.88    -0.34    -0.07     0.20     0.75  4853    1
# betan[30]          -0.14    0.01  0.42    -0.96    -0.41    -0.14     0.13     0.70  5132    1
# betan[31]          -0.10    0.01  0.50    -1.05    -0.44    -0.10     0.24     0.89  4209    1
# betan[32]           0.09    0.01  0.50    -0.88    -0.25     0.08     0.42     1.07  3875    1
# betan[33]           0.14    0.01  0.49    -0.79    -0.19     0.14     0.46     1.13  3870    1
# betan[34]           0.05    0.00  0.26    -0.48    -0.13     0.05     0.23     0.56  4015    1
# betan[35]          -0.02    0.00  0.26    -0.53    -0.20    -0.02     0.15     0.51  3791    1
# betan[36]          -0.13    0.00  0.27    -0.64    -0.32    -0.13     0.05     0.42  4408    1
# betan[37]          -0.10    0.01  0.44    -0.94    -0.39    -0.09     0.20     0.76  4325    1
# betan[38]          -0.02    0.01  0.44    -0.89    -0.32    -0.02     0.28     0.86  4237    1
# betan[39]           0.03    0.01  0.45    -0.85    -0.27     0.03     0.34     0.90  4793    1
# betan[40]          -1.70    0.01  0.36    -2.38    -1.94    -1.69    -1.46    -0.99  2206    1
# betan[41]          -0.73    0.00  0.20    -1.13    -0.87    -0.73    -0.59    -0.34  1851    1
# betan[42]           0.34    0.01  0.48    -0.62     0.02     0.34     0.66     1.29  1113    1
# betan[43]          -0.33    0.01  0.28    -0.88    -0.52    -0.34    -0.15     0.22  2001    1
# betan[44]          -0.64    0.01  0.35    -1.33    -0.87    -0.64    -0.39     0.05  2056    1
# betan[45]          -2.05    0.01  0.32    -2.67    -2.26    -2.06    -1.83    -1.41  1752    1
# betan[46]          -0.94    0.01  0.44    -1.82    -1.24    -0.94    -0.64    -0.09  1635    1
# betan[47]           1.23    0.02  0.52     0.23     0.88     1.22     1.57     2.27  1084    1
# betan[48]          -1.06    0.03  0.84    -2.71    -1.63    -1.07    -0.50     0.57  1049    1
# betan[49]          -0.76    0.01  0.24    -1.23    -0.92    -0.76    -0.61    -0.29  1608    1
# betan[50]           0.19    0.00  0.31    -0.43    -0.02     0.20     0.41     0.80  5710    1
# betan[51]          -0.17    0.01  0.28    -0.72    -0.35    -0.16     0.02     0.37  2635    1
# za[1,1]            -0.04    0.01  0.99    -2.00    -0.72    -0.04     0.61     1.92  8224    1
# za[1,2]             0.01    0.01  0.97    -1.88    -0.63     0.01     0.65     1.92  9909    1
# za[1,3]            -0.02    0.01  1.01    -2.02    -0.70    -0.02     0.67     1.91  8522    1
# za[1,4]             0.00    0.01  1.02    -2.01    -0.67     0.01     0.70     2.04  9041    1
# za[1,5]            -0.01    0.01  1.02    -1.95    -0.74    -0.01     0.71     1.95  9652    1
# za[1,6]            -0.02    0.01  0.99    -1.97    -0.67    -0.03     0.63     1.88  8701    1
# za[1,7]             0.00    0.01  1.01    -2.01    -0.66     0.01     0.66     1.99  7892    1
# za[1,8]             0.00    0.01  0.97    -1.91    -0.64    -0.02     0.65     1.91  9495    1
# za[1,9]             0.00    0.01  0.98    -1.95    -0.67    -0.01     0.66     1.91  8335    1
# za[1,10]           -0.01    0.01  1.01    -1.96    -0.69    -0.01     0.67     1.92 10807    1
# za[1,11]           -0.02    0.01  0.99    -1.91    -0.68    -0.03     0.64     1.89 11596    1
# za[1,12]            0.00    0.01  1.02    -1.96    -0.71    -0.02     0.69     1.98  8097    1
# za[1,13]            0.02    0.01  1.03    -1.96    -0.70     0.02     0.72     2.00  8281    1
# za[1,14]            0.02    0.01  1.01    -1.94    -0.68     0.02     0.71     2.04  8212    1
# za[1,15]           -0.01    0.01  0.97    -1.90    -0.64    -0.02     0.64     1.89  9225    1
# za[1,16]            0.00    0.01  1.02    -1.93    -0.71     0.03     0.68     1.93 11055    1
# za[1,17]            0.02    0.01  1.03    -2.00    -0.68     0.01     0.70     2.05  9017    1
# za[1,18]            0.01    0.01  1.01    -2.01    -0.67     0.02     0.69     1.95  8457    1
# za[1,19]           -0.03    0.01  1.00    -1.99    -0.71    -0.02     0.67     1.90  7558    1
# za[1,20]            0.02    0.01  0.96    -1.86    -0.65     0.03     0.69     1.83  9970    1
# za[1,21]            0.00    0.01  1.02    -1.96    -0.67     0.00     0.70     1.98  9875    1
# za[1,22]            0.00    0.01  0.98    -1.87    -0.70     0.00     0.67     1.92  9818    1
# za[1,23]            0.00    0.01  1.00    -1.93    -0.69     0.00     0.68     1.93  9646    1
# za[1,24]            0.02    0.01  1.01    -1.91    -0.68     0.03     0.69     2.03  9892    1
# za[1,25]            0.02    0.01  1.00    -1.90    -0.64     0.02     0.69     1.97  8703    1
# za[1,26]            0.00    0.01  0.99    -1.92    -0.64     0.01     0.66     1.94  9672    1
# za[1,27]           -0.01    0.01  0.98    -1.93    -0.65    -0.02     0.64     1.91  9001    1
# za[1,28]           -0.02    0.01  1.02    -2.07    -0.70    -0.01     0.66     1.96 10117    1
# za[1,29]           -0.01    0.01  0.97    -1.85    -0.69    -0.02     0.66     1.85  8020    1
# za[1,30]            0.00    0.01  1.02    -1.94    -0.71    -0.01     0.72     2.01 11628    1
# za[1,31]            0.00    0.01  1.01    -2.00    -0.69     0.00     0.69     2.00 10556    1
# za[1,32]            0.00    0.01  0.97    -1.94    -0.64     0.01     0.64     1.86  9242    1
# za[1,33]            0.00    0.01  1.01    -1.96    -0.71     0.01     0.70     1.92  8614    1
# za[1,34]            0.02    0.01  0.97    -1.90    -0.62     0.02     0.66     1.94  8819    1
# za[1,35]            0.03    0.01  0.99    -1.88    -0.65     0.02     0.68     1.97  9387    1
# za[1,36]            0.01    0.01  1.04    -2.01    -0.72     0.00     0.71     2.04  9684    1
# za[1,37]           -0.01    0.01  0.98    -1.89    -0.68    -0.02     0.66     1.88  8496    1
# za[1,38]           -0.02    0.01  1.01    -2.02    -0.70    -0.02     0.67     1.94  8573    1
# za[1,39]           -0.01    0.01  0.99    -1.98    -0.67    -0.01     0.67     1.92  8496    1
# za[1,40]            0.00    0.01  1.01    -2.00    -0.66     0.01     0.68     1.97  9740    1
# za[1,41]            0.00    0.01  1.01    -2.00    -0.66     0.01     0.68     1.97  7684    1
# za[1,42]           -0.03    0.01  0.98    -1.93    -0.69    -0.04     0.64     1.88  7774    1
# za[1,43]            0.01    0.01  1.02    -2.04    -0.68     0.01     0.72     2.05  8563    1
# za[1,44]            0.00    0.01  1.03    -2.04    -0.69     0.00     0.68     2.03  7893    1
# za[1,45]            0.01    0.01  0.99    -1.92    -0.67     0.01     0.69     1.98  9200    1
# za[1,46]            0.03    0.01  1.02    -2.00    -0.66     0.03     0.70     2.06  9063    1
# za[1,47]            0.00    0.01  1.03    -2.03    -0.70     0.03     0.68     1.98 10126    1
# za[1,48]            0.00    0.01  1.00    -1.92    -0.67     0.00     0.69     1.95  9787    1
# za[1,49]           -0.03    0.01  1.01    -2.01    -0.73    -0.02     0.67     1.93  9236    1